{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"COCO to 3D Synthetic Pipeline","text":"<p>Welcome to the documentation for the COCO to 3D Synthetic Pipeline project!</p>"},{"location":"#overview","title":"Overview","text":"<p>This project implements a comprehensive three-stage pipeline for transforming 2D COCO-style instance annotations into 3D models and generating synthetic training images.</p>"},{"location":"#pipeline-stages","title":"Pipeline Stages","text":"<pre><code>graph LR\n    A[COCO Dataset] --&gt; B[COCO Processing]\n    B --&gt; C[3D Generation]\n    C --&gt; D[Synthetic Rendering]\n    D --&gt; E[Synthetic Images]\n</code></pre> <ol> <li>COCO Processing: Load and process COCO-style instance annotation datasets</li> <li>3D Model Generation: Convert 2D annotations to synthetic 3D models</li> <li>Synthetic Image Rendering: Generate new synthetic images from the 3D models</li> </ol>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>COCO Format Support: Full compatibility with COCO-style instance segmentation datasets</li> <li>Automated 3D Generation: Transform 2D masks into 3D mesh representations</li> <li>Flexible Rendering: Generate synthetic images from multiple viewpoints and lighting conditions</li> <li>Extensible Architecture: Modular design for easy customization and extension</li> </ul>"},{"location":"#use-cases","title":"Use Cases","text":"<ul> <li>Data Augmentation: Generate additional training data for object detection and segmentation models</li> <li>Domain Adaptation: Create synthetic datasets for specific domains or scenarios</li> <li>Research: Experiment with different 3D reconstruction and rendering techniques</li> </ul>"},{"location":"#quick-links","title":"Quick Links","text":"<ul> <li>Installation Guide</li> <li>Quick Start Tutorial</li> <li>Pipeline Overview</li> <li>API Reference</li> <li>Research References - SOTA methods for NeRF and Gaussian Splatting</li> </ul>"},{"location":"#project-structure","title":"Project Structure","text":"<pre><code>coco-to-3d-synthetic/\n\u251c\u2500\u2500 data/                  # Data directories\n\u251c\u2500\u2500 src/                   # Source code\n\u2502   \u251c\u2500\u2500 coco_processing/   # COCO processing module\n\u2502   \u251c\u2500\u2500 3d_generation/     # 3D generation module\n\u2502   \u2514\u2500\u2500 synthetic_rendering/ # Rendering module\n\u251c\u2500\u2500 notebooks/             # Jupyter notebooks\n\u2514\u2500\u2500 configs/              # Configuration files\n</code></pre>"},{"location":"#getting-started","title":"Getting Started","text":"<p>To get started with the COCO to 3D Synthetic Pipeline, head over to the Installation Guide to set up your environment.</p>"},{"location":"contributing/","title":"Contributing","text":"<p>Thank you for your interest in contributing to the COCO to 3D Synthetic Pipeline!</p>"},{"location":"contributing/#getting-started","title":"Getting Started","text":"<ol> <li>Fork the repository on GitHub</li> <li>Clone your fork locally</li> <li>Create a new branch for your feature or bugfix</li> <li>Make your changes</li> <li>Submit a pull request</li> </ol>"},{"location":"contributing/#development-setup","title":"Development Setup","text":"<pre><code># Clone your fork\ngit clone https://github.com/YOUR_USERNAME/coco-to-3d-synthetic.git\ncd coco-to-3d-synthetic\n\n# Create virtual environment\npython -m venv venv\nsource venv/bin/activate  # or venv\\Scripts\\activate on Windows\n\n# Install dependencies\npip install -r requirements.txt\n\n# Install development dependencies\npip install pytest black flake8 mypy\n</code></pre>"},{"location":"contributing/#code-style","title":"Code Style","text":"<p>This project follows PEP 8 style guidelines.</p>"},{"location":"contributing/#formatting","title":"Formatting","text":"<p>Use Black for code formatting:</p> <pre><code>black src/\n</code></pre>"},{"location":"contributing/#linting","title":"Linting","text":"<p>Run Flake8 to check for issues:</p> <pre><code>flake8 src/\n</code></pre>"},{"location":"contributing/#type-checking","title":"Type Checking","text":"<p>Use mypy for type checking:</p> <pre><code>mypy src/\n</code></pre>"},{"location":"contributing/#testing","title":"Testing","text":"<p>Write tests for new features and bug fixes.</p>"},{"location":"contributing/#running-tests","title":"Running Tests","text":"<pre><code>pytest tests/\n</code></pre>"},{"location":"contributing/#writing-tests","title":"Writing Tests","text":"<p>Place tests in the <code>tests/</code> directory:</p> <pre><code># tests/test_coco_processing.py\nfrom src.coco_processing import COCOProcessor\n\ndef test_load_annotations():\n    processor = COCOProcessor('test_data/annotations.json')\n    annotations = processor.load_annotations()\n    assert annotations is not None\n    assert 'images' in annotations\n</code></pre>"},{"location":"contributing/#pull-request-process","title":"Pull Request Process","text":"<ol> <li> <p>Create a branch: Use descriptive branch names    <pre><code>git checkout -b feature/add-new-renderer\n</code></pre></p> </li> <li> <p>Make changes: Follow code style guidelines</p> </li> <li> <p>Add tests: Ensure your changes are tested</p> </li> <li> <p>Update documentation: Update relevant docs in <code>docs/</code></p> </li> <li> <p>Commit: Write clear commit messages    <pre><code>git commit -m \"Add support for GLB model export\"\n</code></pre></p> </li> <li> <p>Push: Push to your fork    <pre><code>git push origin feature/add-new-renderer\n</code></pre></p> </li> <li> <p>Submit PR: Open a pull request on GitHub</p> </li> </ol>"},{"location":"contributing/#commit-message-guidelines","title":"Commit Message Guidelines","text":"<p>Use clear, descriptive commit messages:</p> <ul> <li>Use present tense (\"Add feature\" not \"Added feature\")</li> <li>Use imperative mood (\"Move cursor to...\" not \"Moves cursor to...\")</li> <li>Limit first line to 72 characters</li> <li>Reference issues and pull requests when relevant</li> </ul> <p>Examples:</p> <pre><code>Add support for GLB format export\n\nImplement GLB export functionality in ModelGenerator.\nCloses #123\n</code></pre>"},{"location":"contributing/#documentation","title":"Documentation","text":"<p>Update documentation when:</p> <ul> <li>Adding new features</li> <li>Changing APIs</li> <li>Fixing bugs that affect usage</li> </ul> <p>Documentation is in the <code>docs/</code> directory and built with MkDocs.</p>"},{"location":"contributing/#building-docs-locally","title":"Building Docs Locally","text":"<pre><code>mkdocs serve\n</code></pre> <p>Visit http://127.0.0.1:8000 to preview.</p>"},{"location":"contributing/#code-of-conduct","title":"Code of Conduct","text":""},{"location":"contributing/#our-standards","title":"Our Standards","text":"<ul> <li>Be respectful and inclusive</li> <li>Welcome newcomers</li> <li>Focus on constructive feedback</li> <li>Assume good intentions</li> </ul>"},{"location":"contributing/#reporting-issues","title":"Reporting Issues","text":"<p>Report issues or bugs by opening a GitHub issue with:</p> <ul> <li>Clear description</li> <li>Steps to reproduce</li> <li>Expected vs actual behavior</li> <li>Environment details (OS, Python version, etc.)</li> </ul>"},{"location":"contributing/#feature-requests","title":"Feature Requests","text":"<p>We welcome feature requests! Open an issue with:</p> <ul> <li>Clear description of the feature</li> <li>Use cases and motivation</li> <li>Possible implementation approach (optional)</li> </ul>"},{"location":"contributing/#questions","title":"Questions?","text":"<p>Feel free to open an issue for questions or discussion.</p>"},{"location":"contributing/#license","title":"License","text":"<p>By contributing, you agree that your contributions will be licensed under the MIT License.</p>"},{"location":"examples/","title":"Examples","text":"<p>Complete examples demonstrating various use cases of the COCO to 3D Synthetic Pipeline.</p>"},{"location":"examples/#basic-pipeline-example","title":"Basic Pipeline Example","text":"<p>A complete end-to-end pipeline example:</p> <pre><code>from src.coco_processing import COCOProcessor\nfrom src.3d_generation import ModelGenerator\nfrom src.synthetic_rendering import SyntheticRenderer\n\n# Initialize components\nprocessor = COCOProcessor('data/coco/annotations.json')\ngenerator = ModelGenerator(method='midas')\nrenderer = SyntheticRenderer(mode='photorealistic')\n\n# Stage 1: Extract masks from COCO\nmasks = processor.extract_masks(category='car')\n\n# Stage 2: Generate 3D models\nmodels = []\nfor i, mask in enumerate(masks[:10]):  # Process first 10\n    model = generator.generate_from_mask(mask)\n    model_path = f'data/3d_models/car_{i}.obj'\n    generator.save_model(model, model_path)\n    models.append(model_path)\n\n# Stage 3: Render synthetic images\nfor i, model_path in enumerate(models):\n    model = renderer.load_model(model_path)\n    for angle in [0, 90, 180, 270]:\n        image = renderer.render(model, camera_angle=angle)\n        renderer.save_image(\n            image,\n            f'data/synthetic_images/car_{i}_angle_{angle}.png'\n        )\n\nprint(f\"Generated {len(models) * 4} synthetic images!\")\n</code></pre>"},{"location":"examples/#category-specific-processing","title":"Category-Specific Processing","text":"<p>Process only specific object categories:</p> <pre><code>from src.coco_processing import COCOProcessor\n\nprocessor = COCOProcessor('data/coco/annotations.json')\n\n# Process only people\nperson_masks = processor.extract_masks(category='person')\n\n# Process multiple categories\ncategories = ['car', 'bicycle', 'motorcycle']\nfor cat in categories:\n    masks = processor.extract_masks(category=cat)\n    print(f\"Found {len(masks)} {cat} instances\")\n</code></pre>"},{"location":"examples/#multi-viewpoint-dataset-generation","title":"Multi-Viewpoint Dataset Generation","text":"<p>Generate training data from multiple viewpoints:</p> <pre><code>from src.synthetic_rendering import SyntheticRenderer\nimport numpy as np\n\nrenderer = SyntheticRenderer(resolution=(640, 480))\nmodel = renderer.load_model('data/3d_models/object.obj')\n\n# Generate 360-degree views\nnum_views = 36\nangles = np.linspace(0, 360, num_views, endpoint=False)\n\nfor i, angle in enumerate(angles):\n    # Vary camera height and distance\n    height = 2.0 + np.sin(np.radians(angle)) * 0.5\n    distance = 5.0 + np.cos(np.radians(angle)) * 1.0\n\n    image = renderer.render(\n        model,\n        camera_angle=angle,\n        camera_height=height,\n        camera_distance=distance\n    )\n\n    renderer.save_image(image, f'views/view_{i:03d}.png')\n</code></pre>"},{"location":"examples/#custom-lighting-scenarios","title":"Custom Lighting Scenarios","text":"<p>Generate data with different lighting conditions:</p> <pre><code>from src.synthetic_rendering import SyntheticRenderer\n\nrenderer = SyntheticRenderer()\nmodel = renderer.load_model('data/3d_models/object.obj')\n\nlighting_presets = ['studio', 'outdoor', 'dramatic', 'soft']\n\nfor preset in lighting_presets:\n    renderer.set_lighting(type=preset)\n\n    for angle in range(0, 360, 60):\n        image = renderer.render(model, camera_angle=angle)\n        renderer.save_image(\n            image,\n            f'lighting/{preset}_angle_{angle}.png'\n        )\n</code></pre>"},{"location":"examples/#background-variation","title":"Background Variation","text":"<p>Generate images with varied backgrounds:</p> <pre><code>from src.synthetic_rendering import SyntheticRenderer\n\nrenderer = SyntheticRenderer()\nmodel = renderer.load_model('data/3d_models/object.obj')\n\n# Solid color backgrounds\ncolors = [\n    (255, 255, 255),  # White\n    (128, 128, 128),  # Gray\n    (0, 0, 0),        # Black\n    (100, 150, 200)   # Light blue\n]\n\nfor i, color in enumerate(colors):\n    renderer.set_background(color=color)\n    image = renderer.render(model)\n    renderer.save_image(image, f'backgrounds/solid_{i}.png')\n\n# Image backgrounds\nimport glob\nbackground_images = glob.glob('backgrounds/*.jpg')\n\nfor i, bg_path in enumerate(background_images):\n    renderer.set_background(image=bg_path)\n    image = renderer.render(model)\n    renderer.save_image(image, f'backgrounds/composite_{i}.png')\n</code></pre>"},{"location":"examples/#batch-processing","title":"Batch Processing","text":"<p>Efficiently process large datasets:</p> <pre><code>from src.coco_processing import COCOProcessor\nfrom src.3d_generation import ModelGenerator\nfrom src.synthetic_rendering import SyntheticRenderer\nfrom multiprocessing import Pool\n\nprocessor = COCOProcessor('data/coco/annotations.json')\ngenerator = ModelGenerator()\nrenderer = SyntheticRenderer(mode='fast')\n\n# Extract all masks\nall_masks = processor.extract_masks()\n\n# Batch generate 3D models\nmodels = generator.batch_generate(\n    list(all_masks.values()),\n    num_workers=8\n)\n\n# Save models\nmodel_paths = []\nfor i, model in enumerate(models):\n    path = f'data/3d_models/batch_{i}.obj'\n    generator.save_model(model, path)\n    model_paths.append(path)\n\n# Batch render\nimages = renderer.batch_render(\n    model_paths,\n    num_viewpoints=8,\n    num_workers=4\n)\n\nprint(f\"Processed {len(models)} models, generated {len(images)} images\")\n</code></pre>"},{"location":"examples/#depth-map-generation","title":"Depth Map Generation","text":"<p>Generate depth maps for training:</p> <pre><code>from src.synthetic_rendering import SyntheticRenderer\n\nrenderer = SyntheticRenderer()\nmodel = renderer.load_model('data/3d_models/object.obj')\n\n# Render RGB and depth pairs\nfor i in range(10):\n    angle = i * 36\n\n    # RGB image\n    rgb = renderer.render(model, camera_angle=angle)\n    renderer.save_image(rgb, f'pairs/rgb_{i}.png')\n\n    # Depth map\n    depth = renderer.render_depth(model)\n    renderer.save_image(depth, f'pairs/depth_{i}.exr', format='exr')\n</code></pre>"},{"location":"examples/#quality-vs-speed-comparison","title":"Quality vs Speed Comparison","text":"<p>Compare different rendering modes:</p> <pre><code>from src.synthetic_rendering import SyntheticRenderer\nimport time\n\nmodel_path = 'data/3d_models/object.obj'\n\nmodes = ['fast', 'photorealistic']\n\nfor mode in modes:\n    renderer = SyntheticRenderer(mode=mode)\n    model = renderer.load_model(model_path)\n\n    start = time.time()\n    image = renderer.render(model)\n    elapsed = time.time() - start\n\n    renderer.save_image(image, f'comparison/{mode}.png')\n    print(f\"{mode}: {elapsed:.2f}s\")\n</code></pre>"},{"location":"examples/#configuration-based-pipeline","title":"Configuration-Based Pipeline","text":"<p>Use configuration files for reproducible pipelines:</p> <pre><code>import yaml\nfrom src.coco_processing import COCOProcessor\nfrom src.3d_generation import ModelGenerator\nfrom src.synthetic_rendering import SyntheticRenderer\n\n# Load configuration\nwith open('configs/pipeline.yaml', 'r') as f:\n    config = yaml.safe_load(f)\n\n# Initialize with config\nprocessor = COCOProcessor(**config['coco_processing'])\ngenerator = ModelGenerator(**config['3d_generation'])\nrenderer = SyntheticRenderer(**config['synthetic_rendering'])\n\n# Run pipeline\nmasks = processor.extract_masks()\nmodels = generator.batch_generate(list(masks.values()))\nimages = renderer.batch_render(models)\n</code></pre>"},{"location":"examples/#error-handling","title":"Error Handling","text":"<p>Robust pipeline with error handling:</p> <pre><code>from src.coco_processing import COCOProcessor\nfrom src.3d_generation import ModelGenerator\nimport logging\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\nprocessor = COCOProcessor('data/coco/annotations.json')\ngenerator = ModelGenerator()\n\nmasks = processor.extract_masks()\nsuccessful = 0\nfailed = 0\n\nfor i, (instance_id, mask) in enumerate(masks.items()):\n    try:\n        model = generator.generate_from_mask(mask)\n        output_path = f'data/3d_models/model_{instance_id}.obj'\n        generator.save_model(model, output_path)\n        successful += 1\n        logger.info(f\"Processed {i+1}/{len(masks)}: {instance_id}\")\n    except Exception as e:\n        failed += 1\n        logger.error(f\"Failed on {instance_id}: {str(e)}\")\n\nlogger.info(f\"Complete: {successful} successful, {failed} failed\")\n</code></pre>"},{"location":"references/","title":"Research References","text":"<p>State-of-the-art methods in 3D reconstruction, Neural Radiance Fields (NeRF), and 3D Gaussian Splatting organized chronologically.</p>"},{"location":"references/#neural-radiance-fields-nerf","title":"Neural Radiance Fields (NeRF)","text":""},{"location":"references/#2020","title":"2020","text":""},{"location":"references/#nerf-representing-scenes-as-neural-radiance-fields-for-view-synthesis-eccv-2020","title":"NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis (ECCV 2020)","text":"<ul> <li>Authors: Ben Mildenhall, et al. (UC Berkeley, Google Research, UCSD)</li> <li>Paper: arXiv:2003.08934</li> <li>Code: bmild/nerf</li> <li>Project: nerf-website</li> <li>Description: The foundational paper that introduced Neural Radiance Fields. Represents scenes as continuous 5D functions (3D location + 2D viewing direction) optimized with volume rendering. Achieves photorealistic novel view synthesis but requires long training times (hours to days).</li> </ul>"},{"location":"references/#2021","title":"2021","text":""},{"location":"references/#pixelnerf-neural-radiance-fields-from-one-or-few-images-cvpr-2021","title":"pixelNeRF: Neural Radiance Fields from One or Few Images (CVPR 2021)","text":"<ul> <li>Authors: Alex Yu, et al. (UC Berkeley)</li> <li>Paper: arXiv:2012.02190</li> <li>Code: sxyu/pixel-nerf</li> <li>Description: Extends NeRF to work with one or few input images by conditioning on image features. Uses a fully-convolutional architecture that can generalize to novel scenes without per-scene optimization.</li> </ul>"},{"location":"references/#mip-nerf-a-multiscale-representation-for-anti-aliasing-neural-radiance-fields-iccv-2021","title":"Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields (ICCV 2021)","text":"<ul> <li>Authors: Jonathan T. Barron, et al. (Google Research)</li> <li>Paper: arXiv:2103.13415</li> <li>Code: google/mipnerf</li> <li>Description: Addresses aliasing artifacts in NeRF by reasoning about conical frustums instead of rays. Significantly improves rendering quality, especially when rendering at different scales.</li> </ul>"},{"location":"references/#barf-bundle-adjusting-neural-radiance-fields-iccv-2021","title":"BARF: Bundle-Adjusting Neural Radiance Fields (ICCV 2021)","text":"<ul> <li>Authors: Chen-Hsuan Lin, et al. (CMU, Meta AI)</li> <li>Paper: arXiv:2104.06405</li> <li>Code: chenhsuanlin/bundle-adjusting-NeRF</li> <li>Description: Jointly optimizes NeRF and camera poses without requiring accurate camera calibration. Enables NeRF training from imperfect camera poses.</li> </ul>"},{"location":"references/#2022","title":"2022","text":""},{"location":"references/#instant-neural-graphics-primitives-instant-ngp-siggraph-2022","title":"Instant Neural Graphics Primitives (Instant-NGP) (SIGGRAPH 2022)","text":"<ul> <li>Authors: Thomas M\u00fcller, et al. (NVIDIA)</li> <li>Paper: arXiv:2201.05989</li> <li>Code: NVlabs/instant-ngp</li> <li>Description: Revolutionized NeRF training speed using multiresolution hash encoding. Reduces training time from hours to seconds while maintaining high quality. Includes CUDA implementation for real-time performance.</li> </ul>"},{"location":"references/#mip-nerf-360-unbounded-anti-aliased-neural-radiance-fields-cvpr-2022","title":"Mip-NeRF 360: Unbounded Anti-Aliased Neural Radiance Fields (CVPR 2022)","text":"<ul> <li>Authors: Jonathan T. Barron, et al. (Google Research)</li> <li>Paper: arXiv:2111.12077</li> <li>Code: google-research/multinerf</li> <li>Description: Extends Mip-NeRF to unbounded 360-degree scenes. Uses non-linear scene parameterization and online distillation for high-quality unbounded scene reconstruction.</li> </ul>"},{"location":"references/#tensorf-tensorial-radiance-fields-eccv-2022","title":"TensoRF: Tensorial Radiance Fields (ECCV 2022)","text":"<ul> <li>Authors: Anpei Chen, et al. (UCLA, Meta Reality Labs)</li> <li>Paper: arXiv:2203.09517</li> <li>Code: apchenstu/TensoRF</li> <li>Description: Models radiance fields as 4D tensors factorized into compact components. Achieves faster reconstruction with lower memory footprint compared to vanilla NeRF.</li> </ul>"},{"location":"references/#nerf-studio-a-framework-for-neural-radiance-field-development-2022","title":"NeRF-Studio: A Framework for Neural Radiance Field Development (2022)","text":"<ul> <li>Authors: Matthew Tancik, et al. (UC Berkeley, Luma AI)</li> <li>Code: nerfstudio-project/nerfstudio</li> <li>Website: nerf.studio</li> <li>Description: Modular framework for NeRF research and development. Provides viewer, training infrastructure, and implementations of multiple NeRF variants. Industry-standard tool for NeRF experimentation.</li> </ul>"},{"location":"references/#2023","title":"2023","text":""},{"location":"references/#zip-nerf-anti-aliased-grid-based-neural-radiance-fields-iccv-2023","title":"Zip-NeRF: Anti-Aliased Grid-Based Neural Radiance Fields (ICCV 2023)","text":"<ul> <li>Authors: Jonathan T. Barron, et al. (Google Research)</li> <li>Paper: arXiv:2304.06706</li> <li>Description: Combines hash grids (Instant-NGP) with anti-aliasing (Mip-NeRF) for fast, high-quality reconstruction. Achieves state-of-the-art quality with efficient training times.</li> </ul>"},{"location":"references/#k-planes-explicit-radiance-fields-in-space-time-and-appearance-cvpr-2023","title":"K-Planes: Explicit Radiance Fields in Space, Time, and Appearance (CVPR 2023)","text":"<ul> <li>Authors: Sara Fridovich-Keil, et al. (UC Berkeley, Meta AI)</li> <li>Paper: arXiv:2301.10241</li> <li>Code: sarafridov/K-Planes</li> <li>Description: Factorizes 4D spacetime into six planes for efficient dynamic scene reconstruction. Enables modeling of time-varying scenes with explicit grid-based representation.</li> </ul>"},{"location":"references/#nerf-vae-a-geometry-aware-3d-scene-generative-model-icml-2023","title":"NeRF-VAE: A Geometry Aware 3D Scene Generative Model (ICML 2023)","text":"<ul> <li>Authors: Adam R. Kosiorek, et al. (DeepMind)</li> <li>Paper: arXiv:2104.00587</li> <li>Description: Combines NeRF with variational autoencoders for generative 3D scene modeling. Learns disentangled representations of scene geometry and appearance.</li> </ul>"},{"location":"references/#2024","title":"2024","text":""},{"location":"references/#lrm-large-reconstruction-model-for-single-image-to-3d-iclr-2024","title":"LRM: Large Reconstruction Model for Single Image to 3D (ICLR 2024)","text":"<ul> <li>Authors: Yicong Hong, et al. (NVIDIA, Meta AI, University of Hong Kong)</li> <li>Paper: arXiv:2311.04400</li> <li>Code: 3DTopia/OpenLRM (open-source implementation)</li> <li>Description: 500M parameter transformer that predicts triplane NeRF from single images in ~5 seconds. Trained on 1M+ objects from Objaverse. State-of-the-art for fast single-image 3D reconstruction.</li> </ul>"},{"location":"references/#ssdnerf-semantic-aware-single-stage-diffusion-nerf-apple-2024","title":"SSDNeRF: Semantic-aware Single-stage Diffusion NeRF (Apple, 2024)","text":"<ul> <li>Authors: Apple Research</li> <li>Paper: Research paper</li> <li>Description: Unified single-stage training approach combining diffusion models with NeRF. End-to-end optimization for high-quality 3D generation with semantic awareness.</li> </ul>"},{"location":"references/#mvdream-multi-view-diffusion-for-3d-generation-2024","title":"MVDream: Multi-view Diffusion for 3D Generation (2024)","text":"<ul> <li>Authors: Yichun Shi, et al. (ByteDance)</li> <li>Paper: arXiv:2308.16512</li> <li>Code: bytedance/MVDream</li> <li>Description: Multi-view consistent diffusion model for text-to-3D. Trained on both 2D and 3D data to ensure view consistency during generation.</li> </ul>"},{"location":"references/#3d-gaussian-splatting","title":"3D Gaussian Splatting","text":""},{"location":"references/#2023_1","title":"2023","text":""},{"location":"references/#3d-gaussian-splatting-for-real-time-radiance-field-rendering-siggraph-2023","title":"3D Gaussian Splatting for Real-Time Radiance Field Rendering (SIGGRAPH 2023)","text":"<ul> <li>Authors: Bernhard Kerbl, et al. (Inria, Max Planck Institut)</li> <li>Paper: arXiv:2308.04079</li> <li>Code: graphdeco-inria/gaussian-splatting</li> <li>Project: gaussian-splatting-project</li> <li>Description: Groundbreaking method representing scenes as 3D Gaussians instead of neural networks. Achieves real-time rendering (100+ FPS) with quality comparable to NeRF. Revolutionized real-time 3D reconstruction.</li> </ul>"},{"location":"references/#2024_1","title":"2024","text":""},{"location":"references/#splatter-image-ultra-fast-single-view-3d-reconstruction-cvpr-2024","title":"Splatter Image: Ultra-Fast Single-View 3D Reconstruction (CVPR 2024)","text":"<ul> <li>Authors: Stanislaw Szymanowicz, et al. (University of Oxford, Niantic)</li> <li>Paper: arXiv:2312.13150</li> <li>Code: szymanowiczs/splatter-image</li> <li>Description: Maps single image pixels directly to 3D Gaussians using feed-forward network. Ultra-fast inference for real-time single-view 3D reconstruction.</li> </ul>"},{"location":"references/#instantmesh-efficient-3d-mesh-generation-from-a-single-image-with-sparse-view-large-reconstruction-models-2024","title":"InstantMesh: Efficient 3D Mesh Generation from a Single Image with Sparse-view Large Reconstruction Models (2024)","text":"<ul> <li>Authors: Jiale Xu, et al. (Tencent ARC Lab)</li> <li>Paper: arXiv:2404.07191</li> <li>Code: TencentARC/InstantMesh</li> <li>Description: Combines multiview diffusion with sparse-view reconstruction for high-quality textured meshes in ~10 seconds. Superior geometry and texture quality compared to earlier feed-forward methods.</li> </ul>"},{"location":"references/#humansplat-generalizable-single-image-human-gaussian-splatting-with-structure-priors-neurips-2024","title":"HumanSplat: Generalizable Single-Image Human Gaussian Splatting with Structure Priors (NeurIPS 2024)","text":"<ul> <li>Authors: Panwang Pan, et al.</li> <li>Paper: arXiv:2406.12459</li> <li>Project: humansplat.github.io</li> <li>Description: Human-specific Gaussian Splatting using structure priors for anatomically correct reconstruction. State-of-the-art for single-image human digitization with photorealistic novel views.</li> </ul>"},{"location":"references/#fdgaussian-fast-gaussian-splatting-from-single-image-via-geometric-aware-diffusion-model-2024","title":"FDGaussian: Fast Gaussian Splatting from Single Image via Geometric-aware Diffusion Model (2024)","text":"<ul> <li>Authors: Qijun Feng, et al.</li> <li>Paper: arXiv:2403.10242</li> <li>Description: Combines geometry-aware diffusion with Gaussian Splatting. Uses orthogonal plane decomposition and epipolar attention for view-consistent multi-view synthesis.</li> </ul>"},{"location":"references/#lgm-large-multi-view-gaussian-model-for-high-resolution-3d-content-creation-2024","title":"LGM: Large Multi-View Gaussian Model for High-Resolution 3D Content Creation (2024)","text":"<ul> <li>Authors: Jiaxiang Tang, et al.</li> <li>Paper: arXiv:2402.05054</li> <li>Code: 3DTopia/LGM</li> <li>Description: Multi-view Gaussian reconstruction model that replaces memory-intensive volume rendering. Fast rendering but with some multi-view inconsistency challenges.</li> </ul>"},{"location":"references/#gsd-view-guided-gaussian-splatting-diffusion-for-3d-reconstruction-eccv-2024","title":"GSD: View-Guided Gaussian Splatting Diffusion for 3D Reconstruction (ECCV 2024)","text":"<ul> <li>Authors: Yuxuan Mu, et al.</li> <li>Paper: arXiv:2407.04237</li> <li>Description: Integrates diffusion models with Gaussian Splatting for high-quality object reconstruction. View-guided approach ensures consistency across generated views.</li> </ul>"},{"location":"references/#triposr-fast-3d-object-reconstruction-from-a-single-image-2024","title":"TripoSR: Fast 3D Object Reconstruction from a Single Image (2024)","text":"<ul> <li>Authors: Stability AI, Tripo AI</li> <li>Paper: arXiv:2403.02151</li> <li>Code: VAST-AI-Research/TripoSR</li> <li>Description: Feed-forward 3D reconstruction model generating meshes in &lt;0.5 seconds on A100. MIT licensed for commercial use. Excellent speed/quality balance for production use.</li> </ul>"},{"location":"references/#hybrid-diffusion-based-methods","title":"Hybrid &amp; Diffusion-Based Methods","text":""},{"location":"references/#2022_1","title":"2022","text":""},{"location":"references/#dreamfusion-text-to-3d-using-2d-diffusion-2022","title":"DreamFusion: Text-to-3D using 2D Diffusion (2022)","text":"<ul> <li>Authors: Ben Poole, et al. (Google Research, UC Berkeley)</li> <li>Paper: arXiv:2209.14988</li> <li>Code: ashawkey/stable-dreamfusion (unofficial)</li> <li>Description: Pioneering text-to-3D method using Score Distillation Sampling (SDS) with 2D diffusion models as priors. Slow (~1.5 hours) but opened new research direction for text-to-3D generation.</li> </ul>"},{"location":"references/#2023_2","title":"2023","text":""},{"location":"references/#magic3d-high-resolution-text-to-3d-content-creation-cvpr-2023","title":"Magic3D: High-Resolution Text-to-3D Content Creation (CVPR 2023)","text":"<ul> <li>Authors: Chen-Hsuan Lin, et al. (NVIDIA)</li> <li>Paper: arXiv:2211.10440</li> <li>Description: Two-stage coarse-to-fine text-to-3D optimization. Achieves 2x faster generation and 8x higher resolution than DreamFusion through sparse 3D hash grid and differentiable rendering.</li> </ul>"},{"location":"references/#zero-1-to-3-zero-shot-one-image-to-3d-object-iccv-2023","title":"Zero-1-to-3: Zero-shot One Image to 3D Object (ICCV 2023)","text":"<ul> <li>Authors: Ruoshi Liu, et al. (Columbia University)</li> <li>Paper: arXiv:2303.11328</li> <li>Code: cvlab-columbia/zero123</li> <li>Description: Diffusion model for novel view synthesis from single image with camera control. Enables controllable view generation for 3D reconstruction pipelines.</li> </ul>"},{"location":"references/#stable-zero123-stability-ai-2023","title":"Stable Zero123 (Stability AI, 2023)","text":"<ul> <li>Model: stabilityai/stable-zero123</li> <li>Description: Improved version of Zero-1-to-3 with better training data and elevation conditioning. Open-source on Hugging Face, integrated with threestudio framework.</li> </ul>"},{"location":"references/#2024_2","title":"2024","text":""},{"location":"references/#wonder3d-single-image-to-3d-using-cross-domain-diffusion-cvpr-2024","title":"Wonder3D: Single Image to 3D using Cross-Domain Diffusion (CVPR 2024)","text":"<ul> <li>Authors: Xiaoxiao Long, et al.</li> <li>Paper: arXiv:2310.15008</li> <li>Code: xxlong0/Wonder3D</li> <li>Updates: Wonder3D++ and Era3D (512x512 with auto focal length) released Dec 2024</li> <li>Description: Cross-domain diffusion for consistent geometry and texture generation. Produces high-detail textured meshes in 2-3 minutes.</li> </ul>"},{"location":"references/#unique3d-high-quality-and-efficient-3d-mesh-generation-from-a-single-image-neurips-2024","title":"Unique3D: High-Quality and Efficient 3D Mesh Generation from a Single Image (NeurIPS 2024)","text":"<ul> <li>Authors: Kailu Wu, et al. (AiuniAI)</li> <li>Paper: arXiv:2405.20343</li> <li>Code: AiuniAI/Unique3D</li> <li>Description: Single-image to mesh in ~30 seconds using diffusion. High-fidelity diverse meshes that work on wild images with good generalization.</li> </ul>"},{"location":"references/#crm-convolutional-reconstruction-model-eccv-2024","title":"CRM: Convolutional Reconstruction Model (ECCV 2024)","text":"<ul> <li>Authors: Zhaoxi Chen, et al. (Tsinghua University)</li> <li>Paper: arXiv:2403.05034</li> <li>Code: thu-ml/CRM</li> <li>Description: Feed-forward architecture generating textured meshes in ~10 seconds. Direct mesh output without intermediate representations. Good balance of speed and quality.</li> </ul>"},{"location":"references/#sith-single-view-textured-human-reconstruction-with-image-conditioned-diffusion-cvpr-2024","title":"SiTH: Single-view Textured Human Reconstruction with Image-Conditioned Diffusion (CVPR 2024)","text":"<ul> <li>Authors: Hsuan-I Ho, et al.</li> <li>Paper: arXiv:2311.15855</li> <li>Code: SiTH-Diffusion/SiTH</li> <li>Description: Human-specific reconstruction using image-conditioned diffusion. Generates fully textured humans in ~2 minutes with high quality.</li> </ul>"},{"location":"references/#syncdreamer-generating-multiview-consistent-images-from-a-single-view-image-2024","title":"SyncDreamer: Generating Multiview-Consistent Images from a Single-View Image (2024)","text":"<ul> <li>Authors: Yuan Liu, et al.</li> <li>Paper: arXiv:2309.03453</li> <li>Description: 3D-aware feature attention for multi-view consistent generation. Improves upon zero123 with better consistency across views.</li> </ul>"},{"location":"references/#one-2-3-45-any-single-image-to-3d-mesh-in-45-seconds-neurips-2023","title":"One-2-3-45: Any Single Image to 3D Mesh in 45 Seconds (NeurIPS 2023)","text":"<ul> <li>Authors: Minghua Liu, et al. (HKUST, NVIDIA)</li> <li>Paper: arXiv:2306.16928</li> <li>Description: Fast single-image to 3D in 45 seconds using consistent multi-view generation without per-shape optimization. Quality trade-off for speed.</li> </ul>"},{"location":"references/#monocular-depth-estimation","title":"Monocular Depth Estimation","text":""},{"location":"references/#2024-state-of-the-art","title":"2024 State-of-the-Art","text":""},{"location":"references/#depth-anything-unleashing-the-power-of-large-scale-unlabeled-data-cvpr-2024","title":"Depth Anything: Unleashing the Power of Large-Scale Unlabeled Data (CVPR 2024)","text":"<ul> <li>Authors: Lihe Yang, et al. (University of Hong Kong, TikTok)</li> <li>Paper: arXiv:2401.10891</li> <li>Code: LiheYoung/Depth-Anything</li> <li>Model: huggingface.co/depth-anything</li> <li>Description: Winner of MDEC CVPR 2024 challenge with 48.8% improvement. Current state-of-the-art for relative depth estimation, trained on massive unlabeled data.</li> </ul>"},{"location":"references/#depth-pro-sharp-monocular-metric-depth-in-less-than-a-second-apple-2024","title":"Depth Pro: Sharp Monocular Metric Depth in Less Than a Second (Apple, 2024)","text":"<ul> <li>Authors: Apple Research</li> <li>Paper: arXiv:2410.02073</li> <li>Description: Ultra-fast (&lt;1 second) metric depth estimation with sharp boundaries. Orders of magnitude faster than variable-resolution methods.</li> </ul>"},{"location":"references/#metric3d-v2-a-versatile-monocular-geometric-foundation-model-2024","title":"Metric3D v2: A Versatile Monocular Geometric Foundation Model (2024)","text":"<ul> <li>Authors: Wei Yin, et al.</li> <li>Paper: arXiv:2404.15506</li> <li>Description: Rank #1 on multiple benchmarks. Trained on 16M+ images with thousands of camera models. Zero-shot metric depth and surface normals.</li> </ul>"},{"location":"references/#midas-v31-towards-robust-monocular-depth-estimation-2022-2024","title":"MiDaS v3.1: Towards Robust Monocular Depth Estimation (2022-2024)","text":"<ul> <li>Authors: Ren\u00e9 Ranftl, et al. (Intel ISL)</li> <li>Paper: arXiv:1907.01341 (original), ongoing updates</li> <li>Code: isl-org/MiDaS</li> <li>Description: Robust relative depth estimation with model zoo. Industry-standard baseline used by many newer methods. Well-established and widely supported.</li> </ul>"},{"location":"references/#dpt-vision-transformers-for-dense-prediction-iccv-2021","title":"DPT: Vision Transformers for Dense Prediction (ICCV 2021)","text":"<ul> <li>Authors: Ren\u00e9 Ranftl, et al. (Intel ISL)</li> <li>Paper: arXiv:2103.13413</li> <li>Description: Transformer-based dense prediction achieving superior accuracy metrics. Better geometric consistency than many alternatives, part of MiDaS model zoo.</li> </ul>"},{"location":"references/#multi-view-stereo-and-matching","title":"Multi-View Stereo and Matching","text":""},{"location":"references/#2024_3","title":"2024","text":""},{"location":"references/#dust3r-geometric-3d-vision-made-easy-cvpr-2024","title":"DUSt3R: Geometric 3D Vision Made Easy (CVPR 2024)","text":"<ul> <li>Authors: Shuzhe Wang, et al. (Naver Labs Europe)</li> <li>Paper: arXiv:2312.14132</li> <li>Code: naver/dust3r</li> <li>Description: Dense unconstrained stereo reconstruction without camera parameters. Foundational model approach that handles unconstrained images for 3D reconstruction.</li> </ul>"},{"location":"references/#mast3r-matching-and-stereo-3d-reconstruction-eccv-2024","title":"MASt3R: Matching and Stereo 3D Reconstruction (ECCV 2024)","text":"<ul> <li>Authors: Vincent Leroy, et al. (Naver Labs Europe)</li> <li>Paper: arXiv:2406.09756</li> <li>Code: naver/mast3r</li> <li>Description: Built on DUSt3R with dense local features. 30% improvement on challenging datasets. State-of-the-art for matching and metric 3D reconstruction.</li> </ul>"},{"location":"references/#human-specific-reconstruction","title":"Human-Specific Reconstruction","text":""},{"location":"references/#2019-2020","title":"2019-2020","text":""},{"location":"references/#pifu-pixel-aligned-implicit-function-for-high-resolution-clothed-human-digitization-iccv-2019","title":"PIFu: Pixel-Aligned Implicit Function for High-Resolution Clothed Human Digitization (ICCV 2019)","text":"<ul> <li>Authors: Shunsuke Saito, et al. (Meta Reality Labs)</li> <li>Paper: arXiv:1905.05172</li> <li>Code: shunsukesaito/PIFu</li> <li>Description: Pixel-aligned implicit function for high-resolution human digitization. Foundational work for human reconstruction from single images.</li> </ul>"},{"location":"references/#pifuhd-multi-level-pixel-aligned-implicit-function-for-high-resolution-3d-human-digitization-cvpr-2020","title":"PIFuHD: Multi-Level Pixel-Aligned Implicit Function for High-Resolution 3D Human Digitization (CVPR 2020)","text":"<ul> <li>Authors: Shunsuke Saito, et al. (Meta Reality Labs)</li> <li>Paper: arXiv:2004.00452</li> <li>Code: facebookresearch/pifuhd</li> <li>Description: Multi-level architecture (coarse + fine) for high-resolution clothed human reconstruction. No segmentation mask required. Industry standard for human avatars.</li> </ul>"},{"location":"references/#recommended-resources","title":"Recommended Resources","text":""},{"location":"references/#frameworks-and-tools","title":"Frameworks and Tools","text":"<ul> <li>NeRF Studio: docs.nerf.studio - Complete NeRF development framework</li> <li>threestudio: threestudio-project/threestudio - Unified framework for 3D generation</li> <li>Nerfacc: nerfacc - Efficient NeRF acceleration library</li> </ul>"},{"location":"references/#datasets","title":"Datasets","text":"<ul> <li>Objaverse: allenai/objaverse - 1M+ 3D objects</li> <li>MVImgNet: Multi-view images for training</li> <li>ScanNet++: High-quality indoor scene scans</li> <li>CO3Dv2: Common Objects in 3D version 2</li> </ul>"},{"location":"references/#benchmarks-and-challenges","title":"Benchmarks and Challenges","text":"<ul> <li>MDEC Challenge (CVPR 2024): Monocular Depth Estimation Challenge</li> <li>NeRF Synthetic: Standard NeRF benchmark scenes</li> <li>Mip-NeRF 360 Dataset: Unbounded scene evaluation</li> </ul>"},{"location":"references/#latest-trends-2024-2025","title":"Latest Trends (2024-2025)","text":"<ol> <li>Feed-Forward Models: Shift from per-scene optimization to single-pass inference</li> <li>Gaussian Splatting Dominance: Replacing NeRF for real-time applications</li> <li>Transformer Architectures: Large reconstruction models (LRM) with 500M+ parameters</li> <li>Diffusion Integration: Combining 2D diffusion priors with 3D representations</li> <li>Multi-View Consistency: Better view-consistent generation (MVDream, SyncDreamer)</li> <li>Foundation Models: Training on millions of objects for generalization</li> <li>Hybrid Representations: Combining strengths of NeRF, Gaussians, and meshes</li> <li>Camera-Free Reconstruction: DUSt3R/MASt3R eliminate need for camera parameters</li> </ol> <p>Last updated: November 2024</p>"},{"location":"api/3d-generation/","title":"3D Generation API","text":"<p>API reference for the 3D Generation module.</p>"},{"location":"api/3d-generation/#modelgenerator","title":"ModelGenerator","text":"<p>Main class for generating 3D models from 2D masks.</p>"},{"location":"api/3d-generation/#constructor","title":"Constructor","text":"<pre><code>ModelGenerator(\n    method: str = 'midas',\n    resolution: int = 256,\n    device: str = 'cuda'\n)\n</code></pre> <p>Parameters:</p> <ul> <li><code>method</code> (str): Depth estimation method ('midas', 'silhouette')</li> <li><code>resolution</code> (int): Mesh resolution</li> <li><code>device</code> (str): Compute device ('cuda' or 'cpu')</li> </ul> <p>Example:</p> <pre><code>generator = ModelGenerator(method='midas', resolution=512)\n</code></pre>"},{"location":"api/3d-generation/#methods","title":"Methods","text":""},{"location":"api/3d-generation/#generate_from_mask","title":"generate_from_mask()","text":"<p>Generate 3D model from 2D segmentation mask.</p> <pre><code>def generate_from_mask(\n    mask: np.ndarray,\n    depth_scale: float = 1.0,\n    mesh_simplification: float = 1.0\n) -&gt; Mesh3D\n</code></pre> <p>Parameters:</p> <ul> <li><code>mask</code> (np.ndarray): Binary segmentation mask</li> <li><code>depth_scale</code> (float): Scale factor for depth values</li> <li><code>mesh_simplification</code> (float): Simplification ratio (0-1)</li> </ul> <p>Returns: 3D mesh object</p>"},{"location":"api/3d-generation/#save_model","title":"save_model()","text":"<p>Save 3D model to file.</p> <pre><code>def save_model(\n    model: Mesh3D,\n    output_path: str,\n    format: str = 'obj'\n) -&gt; None\n</code></pre> <p>Parameters:</p> <ul> <li><code>model</code> (Mesh3D): 3D mesh to save</li> <li><code>output_path</code> (str): Output file path</li> <li><code>format</code> (str): File format ('obj', 'ply', 'stl', 'gltf')</li> </ul>"},{"location":"api/3d-generation/#batch_generate","title":"batch_generate()","text":"<p>Generate multiple 3D models in batch.</p> <pre><code>def batch_generate(\n    masks: list,\n    num_workers: int = 4\n) -&gt; list\n</code></pre> <p>Parameters:</p> <ul> <li><code>masks</code> (list): List of binary masks</li> <li><code>num_workers</code> (int): Number of parallel workers</li> </ul> <p>Returns: List of 3D mesh objects</p>"},{"location":"api/3d-generation/#estimate_depth","title":"estimate_depth()","text":"<p>Estimate depth map from mask.</p> <pre><code>def estimate_depth(mask: np.ndarray) -&gt; np.ndarray\n</code></pre> <p>Parameters:</p> <ul> <li><code>mask</code> (np.ndarray): Binary segmentation mask</li> </ul> <p>Returns: Depth map as numpy array</p>"},{"location":"api/3d-generation/#simplify_mesh","title":"simplify_mesh()","text":"<p>Reduce mesh polygon count.</p> <pre><code>def simplify_mesh(\n    model: Mesh3D,\n    target_faces: int,\n    preserve_boundary: bool = True\n) -&gt; Mesh3D\n</code></pre> <p>Parameters:</p> <ul> <li><code>model</code> (Mesh3D): Input mesh</li> <li><code>target_faces</code> (int): Target number of faces</li> <li><code>preserve_boundary</code> (bool): Preserve mesh boundaries</li> </ul> <p>Returns: Simplified mesh</p>"},{"location":"api/3d-generation/#smooth_mesh","title":"smooth_mesh()","text":"<p>Apply smoothing to mesh.</p> <pre><code>def smooth_mesh(\n    model: Mesh3D,\n    iterations: int = 10,\n    method: str = 'laplacian'\n) -&gt; Mesh3D\n</code></pre> <p>Parameters:</p> <ul> <li><code>model</code> (Mesh3D): Input mesh</li> <li><code>iterations</code> (int): Number of smoothing iterations</li> <li><code>method</code> (str): Smoothing method ('laplacian', 'taubin')</li> </ul> <p>Returns: Smoothed mesh</p>"},{"location":"api/coco-processing/","title":"COCO Processing API","text":"<p>API reference for the COCO Processing module.</p>"},{"location":"api/coco-processing/#cocoprocessor","title":"COCOProcessor","text":"<p>Main class for processing COCO annotation files.</p>"},{"location":"api/coco-processing/#constructor","title":"Constructor","text":"<pre><code>COCOProcessor(annotation_file: str, image_dir: str = None)\n</code></pre> <p>Parameters:</p> <ul> <li><code>annotation_file</code> (str): Path to COCO annotation JSON file</li> <li><code>image_dir</code> (str, optional): Directory containing images</li> </ul> <p>Example:</p> <pre><code>processor = COCOProcessor('annotations.json', 'images/')\n</code></pre>"},{"location":"api/coco-processing/#methods","title":"Methods","text":""},{"location":"api/coco-processing/#load_annotations","title":"load_annotations()","text":"<p>Load and parse COCO annotations.</p> <pre><code>def load_annotations() -&gt; dict\n</code></pre> <p>Returns: Dictionary containing parsed annotations</p>"},{"location":"api/coco-processing/#extract_masks","title":"extract_masks()","text":"<p>Extract instance segmentation masks.</p> <pre><code>def extract_masks(\n    category: str = None,\n    image_id: int = None\n) -&gt; dict\n</code></pre> <p>Parameters:</p> <ul> <li><code>category</code> (str, optional): Filter by category name</li> <li><code>image_id</code> (int, optional): Filter by image ID</li> </ul> <p>Returns: Dictionary mapping instance IDs to binary masks</p>"},{"location":"api/coco-processing/#get_categories","title":"get_categories()","text":"<p>Get list of all categories in the dataset.</p> <pre><code>def get_categories() -&gt; list\n</code></pre> <p>Returns: List of category dictionaries</p>"},{"location":"api/coco-processing/#filter_by_area","title":"filter_by_area()","text":"<p>Filter annotations by area.</p> <pre><code>def filter_by_area(\n    min_area: float = 0,\n    max_area: float = float('inf')\n) -&gt; list\n</code></pre> <p>Parameters:</p> <ul> <li><code>min_area</code> (float): Minimum area threshold</li> <li><code>max_area</code> (float): Maximum area threshold</li> </ul> <p>Returns: Filtered list of annotations</p>"},{"location":"api/coco-processing/#visualize_annotations","title":"visualize_annotations()","text":"<p>Visualize annotations on image.</p> <pre><code>def visualize_annotations(\n    image_id: int,\n    save_path: str = None,\n    show: bool = True\n) -&gt; np.ndarray\n</code></pre> <p>Parameters:</p> <ul> <li><code>image_id</code> (int): Image ID to visualize</li> <li><code>save_path</code> (str, optional): Path to save visualization</li> <li><code>show</code> (bool): Whether to display the image</li> </ul> <p>Returns: Annotated image as numpy array</p>"},{"location":"api/synthetic-rendering/","title":"Synthetic Rendering API","text":"<p>API reference for the Synthetic Rendering module.</p>"},{"location":"api/synthetic-rendering/#syntheticrenderer","title":"SyntheticRenderer","text":"<p>Main class for rendering synthetic images from 3D models.</p>"},{"location":"api/synthetic-rendering/#constructor","title":"Constructor","text":"<pre><code>SyntheticRenderer(\n    mode: str = 'photorealistic',\n    resolution: tuple = (1920, 1080),\n    device: str = 'cuda'\n)\n</code></pre> <p>Parameters:</p> <ul> <li><code>mode</code> (str): Rendering mode ('photorealistic', 'fast', 'depth')</li> <li><code>resolution</code> (tuple): Output image resolution (width, height)</li> <li><code>device</code> (str): Compute device ('cuda' or 'cpu')</li> </ul> <p>Example:</p> <pre><code>renderer = SyntheticRenderer(mode='fast', resolution=(1024, 768))\n</code></pre>"},{"location":"api/synthetic-rendering/#methods","title":"Methods","text":""},{"location":"api/synthetic-rendering/#load_model","title":"load_model()","text":"<p>Load 3D model from file.</p> <pre><code>def load_model(model_path: str) -&gt; Model3D\n</code></pre> <p>Parameters:</p> <ul> <li><code>model_path</code> (str): Path to 3D model file</li> </ul> <p>Returns: Loaded 3D model object</p>"},{"location":"api/synthetic-rendering/#render","title":"render()","text":"<p>Render 3D model to image.</p> <pre><code>def render(\n    model: Model3D,\n    camera_angle: float = 0,\n    camera_distance: float = 5.0,\n    camera_height: float = 2.0,\n    samples: int = 64\n) -&gt; np.ndarray\n</code></pre> <p>Parameters:</p> <ul> <li><code>model</code> (Model3D): 3D model to render</li> <li><code>camera_angle</code> (float): Camera rotation angle (degrees)</li> <li><code>camera_distance</code> (float): Camera distance from object</li> <li><code>camera_height</code> (float): Camera height</li> <li><code>samples</code> (int): Number of rendering samples</li> </ul> <p>Returns: Rendered image as numpy array</p>"},{"location":"api/synthetic-rendering/#save_image","title":"save_image()","text":"<p>Save rendered image to file.</p> <pre><code>def save_image(\n    image: np.ndarray,\n    output_path: str,\n    format: str = 'png'\n) -&gt; None\n</code></pre> <p>Parameters:</p> <ul> <li><code>image</code> (np.ndarray): Image to save</li> <li><code>output_path</code> (str): Output file path</li> <li><code>format</code> (str): Image format ('png', 'jpg', 'exr', 'tiff')</li> </ul>"},{"location":"api/synthetic-rendering/#set_camera","title":"set_camera()","text":"<p>Set camera position and orientation.</p> <pre><code>def set_camera(\n    position: tuple,\n    look_at: tuple = (0, 0, 0),\n    up_vector: tuple = (0, 1, 0),\n    fov: float = 50\n) -&gt; None\n</code></pre> <p>Parameters:</p> <ul> <li><code>position</code> (tuple): Camera position (x, y, z)</li> <li><code>look_at</code> (tuple): Point camera is looking at</li> <li><code>up_vector</code> (tuple): Camera up direction</li> <li><code>fov</code> (float): Field of view in degrees</li> </ul>"},{"location":"api/synthetic-rendering/#set_lighting","title":"set_lighting()","text":"<p>Configure scene lighting.</p> <pre><code>def set_lighting(\n    type: str = 'studio',\n    intensity: float = 1.0,\n    color: tuple = (255, 255, 255)\n) -&gt; None\n</code></pre> <p>Parameters:</p> <ul> <li><code>type</code> (str): Lighting preset ('studio', 'outdoor', 'dramatic', 'soft')</li> <li><code>intensity</code> (float): Light intensity</li> <li><code>color</code> (tuple): Light color (R, G, B)</li> </ul>"},{"location":"api/synthetic-rendering/#set_background","title":"set_background()","text":"<p>Set rendering background.</p> <pre><code>def set_background(\n    color: tuple = None,\n    image: str = None,\n    mode: str = 'solid'\n) -&gt; None\n</code></pre> <p>Parameters:</p> <ul> <li><code>color</code> (tuple): Background color (R, G, B)</li> <li><code>image</code> (str): Path to background image</li> <li><code>mode</code> (str): Background mode ('solid', 'image', 'random')</li> </ul>"},{"location":"api/synthetic-rendering/#batch_render","title":"batch_render()","text":"<p>Render multiple models in batch.</p> <pre><code>def batch_render(\n    model_paths: list,\n    num_viewpoints: int = 8,\n    num_workers: int = 4\n) -&gt; list\n</code></pre> <p>Parameters:</p> <ul> <li><code>model_paths</code> (list): List of model file paths</li> <li><code>num_viewpoints</code> (int): Number of viewpoints per model</li> <li><code>num_workers</code> (int): Number of parallel workers</li> </ul> <p>Returns: List of rendered images</p>"},{"location":"api/synthetic-rendering/#render_depth","title":"render_depth()","text":"<p>Render depth map.</p> <pre><code>def render_depth(model: Model3D) -&gt; np.ndarray\n</code></pre> <p>Parameters:</p> <ul> <li><code>model</code> (Model3D): 3D model to render</li> </ul> <p>Returns: Depth map as numpy array</p>"},{"location":"api/synthetic-rendering/#render_normals","title":"render_normals()","text":"<p>Render surface normals map.</p> <pre><code>def render_normals(model: Model3D) -&gt; np.ndarray\n</code></pre> <p>Parameters:</p> <ul> <li><code>model</code> (Model3D): 3D model to render</li> </ul> <p>Returns: Normal map as numpy array</p>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>This guide will help you set up the COCO to 3D Synthetic Pipeline on your system.</p>"},{"location":"getting-started/installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.8 or higher</li> <li>Git</li> <li>CUDA-capable GPU (recommended for 3D processing)</li> </ul>"},{"location":"getting-started/installation/#installation-steps","title":"Installation Steps","text":""},{"location":"getting-started/installation/#1-clone-the-repository","title":"1. Clone the Repository","text":"<pre><code>git clone https://github.com/GeorgePearse/coco-to-3d-synthetic.git\ncd coco-to-3d-synthetic\n</code></pre>"},{"location":"getting-started/installation/#2-create-a-virtual-environment","title":"2. Create a Virtual Environment","text":"Linux/macOSWindows <pre><code>python -m venv venv\nsource venv/bin/activate\n</code></pre> <pre><code>python -m venv venv\nvenv\\Scripts\\activate\n</code></pre>"},{"location":"getting-started/installation/#3-install-dependencies","title":"3. Install Dependencies","text":"<pre><code>pip install -r requirements.txt\n</code></pre>"},{"location":"getting-started/installation/#dependencies","title":"Dependencies","text":"<p>The project requires the following main dependencies:</p> <ul> <li>PyTorch: Deep learning framework for 3D processing</li> <li>OpenCV: Computer vision operations</li> <li>pycocotools: COCO dataset utilities</li> <li>NumPy: Numerical computing</li> <li>Pillow: Image processing</li> <li>Matplotlib: Visualization</li> </ul>"},{"location":"getting-started/installation/#verification","title":"Verification","text":"<p>To verify your installation, run:</p> <pre><code>import torch\nimport cv2\nfrom pycocotools.coco import COCO\nprint(\"All dependencies installed successfully!\")\n</code></pre>"},{"location":"getting-started/installation/#gpu-support","title":"GPU Support","text":"<p>For optimal performance with 3D generation, ensure you have CUDA installed:</p> <pre><code>python -c \"import torch; print(f'CUDA available: {torch.cuda.is_available()}')\"\n</code></pre> <p>If CUDA is not available, the pipeline will fall back to CPU processing (slower).</p>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<p>Once installation is complete, proceed to the Quick Start Guide to run your first pipeline.</p>"},{"location":"getting-started/quickstart/","title":"Quick Start","text":"<p>Get up and running with the COCO to 3D Synthetic Pipeline in minutes.</p>"},{"location":"getting-started/quickstart/#step-1-prepare-coco-dataset","title":"Step 1: Prepare COCO Dataset","text":"<p>Place your COCO-format annotation file in the <code>data/coco/</code> directory:</p> <pre><code>data/coco/\n\u251c\u2500\u2500 annotations.json\n\u2514\u2500\u2500 images/\n    \u251c\u2500\u2500 image1.jpg\n    \u251c\u2500\u2500 image2.jpg\n    \u2514\u2500\u2500 ...\n</code></pre>"},{"location":"getting-started/quickstart/#step-2-process-coco-annotations","title":"Step 2: Process COCO Annotations","text":"<pre><code>from src.coco_processing import COCOProcessor\n\n# Initialize processor\nprocessor = COCOProcessor('data/coco/annotations.json')\n\n# Load and process annotations\nannotations = processor.load_annotations()\nmasks = processor.extract_masks()\n</code></pre>"},{"location":"getting-started/quickstart/#step-3-generate-3d-models","title":"Step 3: Generate 3D Models","text":"<pre><code>from src.3d_generation import ModelGenerator\n\n# Initialize 3D generator\ngenerator = ModelGenerator()\n\n# Convert masks to 3D models\nfor instance_id, mask in masks.items():\n    model_3d = generator.generate_from_mask(mask)\n    generator.save_model(model_3d, f'data/3d_models/model_{instance_id}.obj')\n</code></pre>"},{"location":"getting-started/quickstart/#step-4-render-synthetic-images","title":"Step 4: Render Synthetic Images","text":"<pre><code>from src.synthetic_rendering import SyntheticRenderer\n\n# Initialize renderer\nrenderer = SyntheticRenderer()\n\n# Load 3D model and render from different viewpoints\nmodel = renderer.load_model('data/3d_models/model_001.obj')\nfor angle in range(0, 360, 45):\n    image = renderer.render(model, angle=angle)\n    renderer.save_image(image, f'data/synthetic_images/synthetic_{angle}.png')\n</code></pre>"},{"location":"getting-started/quickstart/#example-pipeline","title":"Example Pipeline","text":"<p>Here's a complete example that runs the full pipeline:</p> <pre><code>from src.coco_processing import COCOProcessor\nfrom src.3d_generation import ModelGenerator\nfrom src.synthetic_rendering import SyntheticRenderer\n\n# Stage 1: COCO Processing\nprocessor = COCOProcessor('data/coco/annotations.json')\nmasks = processor.extract_masks()\n\n# Stage 2: 3D Generation\ngenerator = ModelGenerator()\nmodels_3d = [generator.generate_from_mask(mask) for mask in masks]\n\n# Stage 3: Synthetic Rendering\nrenderer = SyntheticRenderer()\nfor i, model in enumerate(models_3d):\n    for angle in [0, 90, 180, 270]:\n        image = renderer.render(model, angle=angle)\n        renderer.save_image(image, f'data/synthetic_images/model_{i}_angle_{angle}.png')\n\nprint(f\"Generated {len(models_3d) * 4} synthetic images!\")\n</code></pre>"},{"location":"getting-started/quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Learn more about COCO Processing</li> <li>Explore 3D Generation options</li> <li>Customize Synthetic Rendering</li> </ul>"},{"location":"pipeline/3d-generation/","title":"3D Generation","text":"<p>The 3D Generation module converts 2D instance segmentation masks into 3D mesh models.</p>"},{"location":"pipeline/3d-generation/#overview","title":"Overview","text":"<p>This module implements various techniques to reconstruct 3D models from 2D segmentation masks, including depth estimation, mesh generation, and texture mapping.</p>"},{"location":"pipeline/3d-generation/#techniques","title":"Techniques","text":""},{"location":"pipeline/3d-generation/#1-depth-estimation","title":"1. Depth Estimation","text":"<p>Estimate depth maps from 2D masks using various methods:</p> <ul> <li>MiDaS: Monocular depth estimation</li> <li>Shape-from-Silhouette: Classical computer vision approach</li> <li>Learning-based: Neural network depth prediction</li> </ul>"},{"location":"pipeline/3d-generation/#2-mesh-generation","title":"2. Mesh Generation","text":"<p>Convert depth maps to 3D meshes:</p> <ul> <li>Marching Cubes: Isosurface extraction</li> <li>Poisson Surface Reconstruction: Smooth mesh generation</li> <li>Delaunay Triangulation: Point cloud to mesh</li> </ul>"},{"location":"pipeline/3d-generation/#3-texture-mapping","title":"3. Texture Mapping","text":"<p>Apply textures to 3D models:</p> <ul> <li>UV Mapping: Project 2D image onto 3D surface</li> <li>Color Transfer: Extract colors from source images</li> <li>Procedural Textures: Generate synthetic textures</li> </ul>"},{"location":"pipeline/3d-generation/#usage","title":"Usage","text":""},{"location":"pipeline/3d-generation/#basic-3d-generation","title":"Basic 3D Generation","text":"<pre><code>from src.3d_generation import ModelGenerator\n\ngenerator = ModelGenerator(method='midas')\n\n# Generate 3D model from mask\nmodel_3d = generator.generate_from_mask(mask)\n\n# Save to file\ngenerator.save_model(model_3d, 'data/3d_models/output.obj')\n</code></pre>"},{"location":"pipeline/3d-generation/#advanced-options","title":"Advanced Options","text":"<pre><code># Configure generation parameters\ngenerator = ModelGenerator(\n    method='midas',\n    resolution=512,\n    smoothing=True,\n    texture_mapping=True\n)\n\n# Generate with custom settings\nmodel_3d = generator.generate_from_mask(\n    mask,\n    depth_scale=1.5,\n    mesh_simplification=0.8\n)\n</code></pre>"},{"location":"pipeline/3d-generation/#batch-processing","title":"Batch Processing","text":"<pre><code># Process multiple masks\nmasks = [mask1, mask2, mask3]\nmodels = generator.batch_generate(masks, num_workers=4)\n\nfor i, model in enumerate(models):\n    generator.save_model(model, f'data/3d_models/model_{i}.obj')\n</code></pre>"},{"location":"pipeline/3d-generation/#depth-estimation-methods","title":"Depth Estimation Methods","text":""},{"location":"pipeline/3d-generation/#midas","title":"MiDaS","text":"<pre><code>generator = ModelGenerator(method='midas')\ndepth_map = generator.estimate_depth(mask)\n</code></pre> <p>Pros: High-quality depth estimation Cons: Requires GPU, slower</p>"},{"location":"pipeline/3d-generation/#shape-from-silhouette","title":"Shape-from-Silhouette","text":"<pre><code>generator = ModelGenerator(method='silhouette')\ndepth_map = generator.estimate_depth(mask)\n</code></pre> <p>Pros: Fast, CPU-friendly Cons: Less accurate depth</p>"},{"location":"pipeline/3d-generation/#mesh-formats","title":"Mesh Formats","text":"<p>Supported output formats:</p> <ul> <li>OBJ: Wavefront OBJ (with MTL for materials)</li> <li>PLY: Polygon File Format</li> <li>STL: Stereolithography (3D printing)</li> <li>GLTF: GL Transmission Format (web-ready)</li> </ul> <pre><code># Export in different formats\ngenerator.save_model(model, 'output.obj', format='obj')\ngenerator.save_model(model, 'output.ply', format='ply')\ngenerator.save_model(model, 'output.stl', format='stl')\n</code></pre>"},{"location":"pipeline/3d-generation/#optimization","title":"Optimization","text":""},{"location":"pipeline/3d-generation/#mesh-simplification","title":"Mesh Simplification","text":"<p>Reduce polygon count while preserving shape:</p> <pre><code>simplified_model = generator.simplify_mesh(\n    model_3d,\n    target_faces=1000,\n    preserve_boundary=True\n)\n</code></pre>"},{"location":"pipeline/3d-generation/#smoothing","title":"Smoothing","text":"<p>Apply smoothing algorithms:</p> <pre><code>smooth_model = generator.smooth_mesh(\n    model_3d,\n    iterations=10,\n    method='laplacian'\n)\n</code></pre>"},{"location":"pipeline/3d-generation/#configuration","title":"Configuration","text":"<p>Configure 3D generation via <code>configs/3d_generation.yaml</code>:</p> <pre><code>3d_generation:\n  # Depth estimation\n  depth_estimator: \"midas\"\n  depth_model: \"DPT_Large\"\n\n  # Mesh generation\n  mesh_resolution: 256\n  marching_cubes_threshold: 0.5\n\n  # Optimization\n  simplification_ratio: 0.8\n  smoothing_iterations: 5\n\n  # Texture\n  texture_resolution: 1024\n  uv_unwrap_method: \"smart_project\"\n</code></pre>"},{"location":"pipeline/3d-generation/#performance-tips","title":"Performance Tips","text":"<ul> <li>Use GPU for depth estimation when available</li> <li>Batch process multiple masks together</li> <li>Cache depth maps to avoid recomputation</li> <li>Adjust resolution based on your needs (lower = faster)</li> </ul>"},{"location":"pipeline/3d-generation/#api-reference","title":"API Reference","text":"<p>For detailed API documentation, see the 3D Generation API Reference.</p>"},{"location":"pipeline/3d-generation/#research-and-state-of-the-art-methods","title":"Research and State-of-the-Art Methods","text":"<p>For comprehensive information on the latest research in 3D reconstruction, including:</p> <ul> <li>Neural Radiance Fields (NeRF) methods</li> <li>3D Gaussian Splatting techniques</li> <li>Diffusion-based approaches</li> <li>Monocular depth estimation models</li> </ul> <p>See the Research References page for papers, code repositories, and detailed comparisons of state-of-the-art methods.</p>"},{"location":"pipeline/3d-generation/#next-steps","title":"Next Steps","text":"<p>After generating 3D models, move on to Synthetic Rendering to create synthetic images.</p>"},{"location":"pipeline/coco-processing/","title":"COCO Processing","text":"<p>The COCO Processing module handles loading, parsing, and extracting data from COCO-format annotation files.</p>"},{"location":"pipeline/coco-processing/#overview","title":"Overview","text":"<p>COCO (Common Objects in Context) is a widely-used dataset format for object detection, segmentation, and captioning tasks. This module provides tools to work with COCO-format data.</p>"},{"location":"pipeline/coco-processing/#features","title":"Features","text":"<ul> <li>Load COCO annotation JSON files</li> <li>Extract instance segmentation masks</li> <li>Filter by category or image</li> <li>Convert annotations to standard formats</li> <li>Validate dataset integrity</li> </ul>"},{"location":"pipeline/coco-processing/#usage","title":"Usage","text":""},{"location":"pipeline/coco-processing/#basic-loading","title":"Basic Loading","text":"<pre><code>from src.coco_processing import COCOProcessor\n\nprocessor = COCOProcessor('data/coco/annotations.json')\nannotations = processor.load_annotations()\n</code></pre>"},{"location":"pipeline/coco-processing/#extracting-masks","title":"Extracting Masks","text":"<pre><code># Extract all instance masks\nmasks = processor.extract_masks()\n\n# Extract masks for specific category\nperson_masks = processor.extract_masks(category='person')\n\n# Extract masks for specific image\nimage_masks = processor.extract_masks(image_id=12345)\n</code></pre>"},{"location":"pipeline/coco-processing/#category-information","title":"Category Information","text":"<pre><code># Get all categories\ncategories = processor.get_categories()\n\n# Get instances by category\ncars = processor.get_instances_by_category('car')\n</code></pre>"},{"location":"pipeline/coco-processing/#coco-format-structure","title":"COCO Format Structure","text":"<p>A typical COCO annotation file has the following structure:</p> <pre><code>{\n  \"images\": [\n    {\n      \"id\": 1,\n      \"file_name\": \"image1.jpg\",\n      \"width\": 640,\n      \"height\": 480\n    }\n  ],\n  \"annotations\": [\n    {\n      \"id\": 1,\n      \"image_id\": 1,\n      \"category_id\": 1,\n      \"segmentation\": [[x1, y1, x2, y2, ...]],\n      \"area\": 1234.5,\n      \"bbox\": [x, y, width, height]\n    }\n  ],\n  \"categories\": [\n    {\n      \"id\": 1,\n      \"name\": \"person\",\n      \"supercategory\": \"person\"\n    }\n  ]\n}\n</code></pre>"},{"location":"pipeline/coco-processing/#advanced-features","title":"Advanced Features","text":""},{"location":"pipeline/coco-processing/#filtering","title":"Filtering","text":"<pre><code># Filter by minimum area\nlarge_objects = processor.filter_by_area(min_area=1000)\n\n# Filter by bounding box size\nfiltered = processor.filter_by_bbox(min_width=50, min_height=50)\n</code></pre>"},{"location":"pipeline/coco-processing/#visualization","title":"Visualization","text":"<pre><code># Visualize annotations on image\nprocessor.visualize_annotations(image_id=12345, save_path='output.png')\n\n# Show mask overlay\nprocessor.show_mask_overlay(annotation_id=67890)\n</code></pre>"},{"location":"pipeline/coco-processing/#configuration","title":"Configuration","text":"<p>Configure COCO processing via <code>configs/coco_processing.yaml</code>:</p> <pre><code>coco_processing:\n  # Filter settings\n  min_area: 100\n  max_area: 50000\n\n  # Category filtering\n  categories: [\"person\", \"car\", \"bicycle\"]\n\n  # Image filtering\n  min_image_width: 640\n  min_image_height: 480\n\n  # Preprocessing\n  normalize_masks: true\n  resize_to: [256, 256]\n</code></pre>"},{"location":"pipeline/coco-processing/#api-reference","title":"API Reference","text":"<p>For detailed API documentation, see the COCO Processing API Reference.</p>"},{"location":"pipeline/coco-processing/#next-steps","title":"Next Steps","text":"<p>Once you have extracted masks from COCO annotations, proceed to 3D Generation to create 3D models.</p>"},{"location":"pipeline/overview/","title":"Pipeline Overview","text":"<p>The COCO to 3D Synthetic Pipeline consists of three main stages, each designed to transform data from one representation to another.</p>"},{"location":"pipeline/overview/#architecture","title":"Architecture","text":"<pre><code>graph TB\n    A[COCO Dataset] --&gt; B[Stage 1: COCO Processing]\n    B --&gt; C[Instance Masks]\n    C --&gt; D[Stage 2: 3D Generation]\n    D --&gt; E[3D Mesh Models]\n    E --&gt; F[Stage 3: Synthetic Rendering]\n    F --&gt; G[Synthetic Images]\n\n    style B fill:#e1f5ff\n    style D fill:#ffe1f5\n    style F fill:#f5ffe1\n</code></pre>"},{"location":"pipeline/overview/#stage-1-coco-processing","title":"Stage 1: COCO Processing","text":"<p>Input: COCO-format annotation JSON file Output: Instance segmentation masks</p> <p>The COCO Processing stage handles:</p> <ul> <li>Loading and parsing COCO annotation files</li> <li>Extracting instance segmentation masks</li> <li>Organizing data by category</li> <li>Preprocessing and normalization</li> </ul> <p>Learn more about COCO Processing \u2192</p>"},{"location":"pipeline/overview/#stage-2-3d-generation","title":"Stage 2: 3D Generation","text":"<p>Input: 2D instance segmentation masks Output: 3D mesh models (OBJ, PLY formats)</p> <p>The 3D Generation stage performs:</p> <ul> <li>Depth estimation from 2D masks</li> <li>3D mesh reconstruction</li> <li>Texture mapping</li> <li>Model optimization and smoothing</li> </ul> <p>Learn more about 3D Generation \u2192</p>"},{"location":"pipeline/overview/#stage-3-synthetic-rendering","title":"Stage 3: Synthetic Rendering","text":"<p>Input: 3D mesh models Output: Rendered synthetic images</p> <p>The Synthetic Rendering stage creates:</p> <ul> <li>Multiple viewpoint renders</li> <li>Varied lighting conditions</li> <li>Different background scenes</li> <li>Augmented training data</li> </ul> <p>Learn more about Synthetic Rendering \u2192</p>"},{"location":"pipeline/overview/#data-flow","title":"Data Flow","text":"<ol> <li>COCO Annotations \u2192 Parse JSON structure</li> <li>Instance Masks \u2192 Extract binary masks for each object</li> <li>3D Reconstruction \u2192 Generate mesh from mask contours</li> <li>Texture Application \u2192 Apply colors/textures to 3D model</li> <li>Scene Setup \u2192 Position camera, lights, and background</li> <li>Rendering \u2192 Generate final synthetic images</li> </ol>"},{"location":"pipeline/overview/#configuration","title":"Configuration","text":"<p>Each stage can be configured independently through YAML configuration files in the <code>configs/</code> directory:</p> <pre><code># configs/pipeline.yaml\ncoco_processing:\n  min_area: 100\n  categories: [\"person\", \"car\", \"bicycle\"]\n\n3d_generation:\n  depth_estimator: \"midas\"\n  mesh_resolution: 256\n\nsynthetic_rendering:\n  num_viewpoints: 8\n  lighting: \"studio\"\n  background: \"random\"\n</code></pre>"},{"location":"pipeline/overview/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>GPU Acceleration: 3D generation and rendering benefit significantly from GPU</li> <li>Batch Processing: Process multiple instances in parallel</li> <li>Caching: Cache intermediate results to avoid recomputation</li> <li>Memory Management: Large datasets may require streaming/chunking</li> </ul>"},{"location":"pipeline/overview/#next-steps","title":"Next Steps","text":"<p>Dive deeper into each stage:</p> <ul> <li>COCO Processing Details</li> <li>3D Generation Techniques</li> <li>Synthetic Rendering Options</li> </ul>"},{"location":"pipeline/synthetic-rendering/","title":"Synthetic Rendering","text":"<p>The Synthetic Rendering module creates realistic synthetic images from 3D models using various rendering techniques.</p>"},{"location":"pipeline/synthetic-rendering/#overview","title":"Overview","text":"<p>This module provides tools to render 3D models from multiple viewpoints, with different lighting conditions and backgrounds, to generate diverse synthetic training data.</p>"},{"location":"pipeline/synthetic-rendering/#features","title":"Features","text":"<ul> <li>Multi-viewpoint rendering</li> <li>Configurable lighting setups</li> <li>Background composition</li> <li>Camera control</li> <li>Post-processing effects</li> </ul>"},{"location":"pipeline/synthetic-rendering/#usage","title":"Usage","text":""},{"location":"pipeline/synthetic-rendering/#basic-rendering","title":"Basic Rendering","text":"<pre><code>from src.synthetic_rendering import SyntheticRenderer\n\nrenderer = SyntheticRenderer()\n\n# Load 3D model\nmodel = renderer.load_model('data/3d_models/model.obj')\n\n# Render from default viewpoint\nimage = renderer.render(model)\n\n# Save result\nrenderer.save_image(image, 'data/synthetic_images/render.png')\n</code></pre>"},{"location":"pipeline/synthetic-rendering/#multi-viewpoint-rendering","title":"Multi-Viewpoint Rendering","text":"<pre><code># Render from multiple angles\nfor angle in range(0, 360, 45):\n    image = renderer.render(\n        model,\n        camera_angle=angle,\n        camera_distance=5.0,\n        camera_height=2.0\n    )\n    renderer.save_image(image, f'output_{angle}.png')\n</code></pre>"},{"location":"pipeline/synthetic-rendering/#lighting-configuration","title":"Lighting Configuration","text":"<pre><code># Configure lighting\nrenderer.set_lighting(\n    type='studio',  # or 'outdoor', 'dramatic', 'soft'\n    intensity=1.2,\n    color=(255, 255, 240)\n)\n\n# Multiple light sources\nrenderer.add_light(position=(5, 5, 5), intensity=1.0, type='point')\nrenderer.add_light(position=(-5, 5, 5), intensity=0.5, type='point')\n</code></pre>"},{"location":"pipeline/synthetic-rendering/#background-composition","title":"Background Composition","text":"<pre><code># Solid color background\nrenderer.set_background(color=(255, 255, 255))\n\n# Image background\nrenderer.set_background(image='backgrounds/outdoor.jpg')\n\n# Random backgrounds\nrenderer.set_background(mode='random', dataset='backgrounds/')\n</code></pre>"},{"location":"pipeline/synthetic-rendering/#rendering-modes","title":"Rendering Modes","text":""},{"location":"pipeline/synthetic-rendering/#photorealistic-rendering","title":"Photorealistic Rendering","text":"<pre><code>renderer = SyntheticRenderer(mode='photorealistic')\nimage = renderer.render(model, samples=128, denoising=True)\n</code></pre> <p>Use case: High-quality visualization, evaluation datasets</p>"},{"location":"pipeline/synthetic-rendering/#fast-rendering","title":"Fast Rendering","text":"<pre><code>renderer = SyntheticRenderer(mode='fast')\nimage = renderer.render(model, samples=16)\n</code></pre> <p>Use case: Quick previews, large-scale data generation</p>"},{"location":"pipeline/synthetic-rendering/#depth-rendering","title":"Depth Rendering","text":"<pre><code>depth_map = renderer.render_depth(model)\n</code></pre> <p>Use case: Depth estimation training data</p>"},{"location":"pipeline/synthetic-rendering/#normal-mapping","title":"Normal Mapping","text":"<pre><code>normal_map = renderer.render_normals(model)\n</code></pre> <p>Use case: Surface normal estimation</p>"},{"location":"pipeline/synthetic-rendering/#camera-control","title":"Camera Control","text":""},{"location":"pipeline/synthetic-rendering/#manual-camera-positioning","title":"Manual Camera Positioning","text":"<pre><code>renderer.set_camera(\n    position=(x, y, z),\n    look_at=(0, 0, 0),\n    up_vector=(0, 1, 0),\n    fov=50\n)\n</code></pre>"},{"location":"pipeline/synthetic-rendering/#automatic-camera-placement","title":"Automatic Camera Placement","text":"<pre><code># Orbit around object\npositions = renderer.generate_orbit_positions(\n    num_views=16,\n    radius=5.0,\n    height_variation=True\n)\n\nfor i, pos in enumerate(positions):\n    renderer.set_camera(position=pos)\n    image = renderer.render(model)\n    renderer.save_image(image, f'orbit_{i}.png')\n</code></pre>"},{"location":"pipeline/synthetic-rendering/#batch-rendering","title":"Batch Rendering","text":"<p>Process multiple models efficiently:</p> <pre><code>models = [\n    'data/3d_models/model1.obj',\n    'data/3d_models/model2.obj',\n    'data/3d_models/model3.obj'\n]\n\n# Batch render with parallel processing\nimages = renderer.batch_render(\n    models,\n    num_viewpoints=8,\n    num_workers=4\n)\n</code></pre>"},{"location":"pipeline/synthetic-rendering/#post-processing","title":"Post-Processing","text":"<p>Apply effects after rendering:</p> <pre><code># Add noise and blur for realism\nimage = renderer.render(model)\nimage = renderer.add_noise(image, intensity=0.02)\nimage = renderer.add_motion_blur(image, strength=0.5)\nimage = renderer.adjust_exposure(image, factor=1.2)\n</code></pre>"},{"location":"pipeline/synthetic-rendering/#configuration","title":"Configuration","text":"<p>Configure rendering via <code>configs/synthetic_rendering.yaml</code>:</p> <pre><code>synthetic_rendering:\n  # Rendering settings\n  resolution: [1920, 1080]\n  samples: 64\n  max_bounces: 8\n\n  # Camera settings\n  fov: 50\n  camera_distance: 5.0\n  num_viewpoints: 16\n\n  # Lighting\n  lighting_type: \"studio\"\n  ambient_intensity: 0.3\n\n  # Background\n  background_mode: \"random\"\n  background_dataset: \"backgrounds/\"\n\n  # Post-processing\n  denoise: true\n  bloom: false\n  motion_blur: false\n</code></pre>"},{"location":"pipeline/synthetic-rendering/#output-formats","title":"Output Formats","text":"<p>Supported image formats:</p> <ul> <li>PNG (lossless)</li> <li>JPEG (compressed)</li> <li>EXR (HDR, 32-bit)</li> <li>TIFF (high-quality)</li> </ul> <pre><code>renderer.save_image(image, 'output.png', format='png')\nrenderer.save_image(image, 'output.exr', format='exr', bit_depth=32)\n</code></pre>"},{"location":"pipeline/synthetic-rendering/#performance-optimization","title":"Performance Optimization","text":""},{"location":"pipeline/synthetic-rendering/#gpu-acceleration","title":"GPU Acceleration","text":"<pre><code># Use GPU for rendering\nrenderer = SyntheticRenderer(device='cuda')\n</code></pre>"},{"location":"pipeline/synthetic-rendering/#tiling-for-large-images","title":"Tiling for Large Images","text":"<pre><code># Render large images in tiles\nimage = renderer.render_tiled(\n    model,\n    resolution=(4096, 4096),\n    tile_size=512\n)\n</code></pre>"},{"location":"pipeline/synthetic-rendering/#caching","title":"Caching","text":"<pre><code># Cache loaded models\nrenderer.enable_model_cache(max_size=10)\n</code></pre>"},{"location":"pipeline/synthetic-rendering/#api-reference","title":"API Reference","text":"<p>For detailed API documentation, see the Synthetic Rendering API Reference.</p>"},{"location":"pipeline/synthetic-rendering/#examples","title":"Examples","text":"<p>Check out the Examples page for complete rendering pipelines and use cases.</p>"}]}